import weaviate  # type: ignore
from weaviate.auth import AuthApiKey
from confluent_kafka import Consumer
from qwen3 import get_embedding  # âœ… ä½ çš„ embedding æ–¹æ³•
import hashlib
import json
import uuid
import redis
from concurrent.futures import ThreadPoolExecutor
import threading
from dotenv import load_dotenv
import os

# âœ… åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# âœ… ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
WEAVIATE_URL = os.getenv("WEAVIATE_URL222")
WEAVIATE_API_KEY = os.getenv("WEAVIATE_API_KEY")

KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS")
KAFKA_GROUP_ID = os.getenv("KAFKA_GROUP_ID")
KAFKA_TOPIC = os.getenv("KAFKA_TOPIC")

REDIS_HOST = os.getenv("REDIS_HOST")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_DB = int(os.getenv("REDIS_DB", 0))

# âœ… Redis å®¢æˆ·ç«¯
redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB, decode_responses=True)

cache_lock = threading.Lock()

def md5_text(text: str) -> str:
    """è®¡ç®—æ–‡æœ¬ MD5"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def split_text_to_paragraphs(text: str, max_len: int = 500) -> list[str]:
    """ç®€å•åˆ‡æ®µï¼Œå¯åç»­æ”¹è¿›"""
    paragraphs = []
    temp = ""
    for line in text.split("\n"):
        if not line.strip():
            continue
        temp += line.strip() + " "
        if len(temp) >= max_len:
            paragraphs.append(temp.strip())
            temp = ""
    if temp:
        paragraphs.append(temp.strip())
    return paragraphs

def process_and_insert(client: weaviate.Client, paragraph: str, meta: dict):
    """å¤„ç†æ®µè½å¹¶å†™å…¥ Weaviate"""
    md5_value = md5_text(paragraph)
    file_id = meta.get("file_id")
    cache_key = f"paragraph_md5_cache:{file_id}"

    with cache_lock:
        if redis_client.sismember(cache_key, md5_value):
            print(f"ğŸ” å·²å­˜åœ¨ï¼Œfile_id={file_id}, è·³è¿‡: {md5_value}")
            return
        redis_client.sadd(cache_key, md5_value)

    # ç”Ÿæˆå‘é‡
    vector = get_embedding(paragraph)

    # æ‹¼æ¥å±æ€§
    properties = {
        "part_id": str(uuid.uuid4()),
        "knowledge_id": meta.get("knowledge_id"),
        "file_id": file_id,
        "userid": meta.get("userid"),
        "username": meta.get("username"),
        "digital_human_id": meta.get("digital_human_id"),
        "part_cntt": paragraph
    }

    try:
        client.batch.add_data_object(
            data_object=properties,
            class_name="KnowledgeParagraph",
            vector=vector,
        )
        print("âœ… å·²å†™å…¥:", properties["part_id"])
    except Exception as e:
        print("âŒ å†™å…¥å¤±è´¥:", e)

def consume_kafka_messages(client: weaviate.Client, topic: str, bootstrap_servers: str, group_id: str):
    """æ¶ˆè´¹ Kafka æ¶ˆæ¯ï¼Œè§£æå¹¶å†™å…¥ Weaviate"""
    consumer = Consumer({
        'bootstrap.servers': bootstrap_servers,
        'group.id': group_id,
        'auto.offset.reset': 'earliest'
    })
    consumer.subscribe([topic])

    executor = ThreadPoolExecutor(max_workers=5)

    try:
        while True:
            msg = consumer.poll(1.0)
            if msg is None:
                continue
            if msg.error():
                print("Kafka é”™è¯¯:", msg.error())
                continue

            msg_json = json.loads(msg.value().decode('utf-8'))

            meta = {
                "knowledge_id": msg_json.get("knowledge_id"),
                "file_id": msg_json.get("file_id"),
                "userid": msg_json.get("userid"),
                "username": msg_json.get("username"),
                "digital_human_id": msg_json.get("digital_human_id")
            }
            content = msg_json.get("content", "")

            paragraphs = split_text_to_paragraphs(content)

            futures = []
            with client.batch as batch:
                for paragraph in paragraphs:
                    future = executor.submit(process_and_insert, client, paragraph, meta)
                    futures.append(future)

                for future in futures:
                    future.result()

    except KeyboardInterrupt:
        print("â›”ï¸ åœæ­¢æ¶ˆè´¹")
    finally:
        consumer.close()
        executor.shutdown()

if __name__ == "__main__":
    # âœ… Weaviate é…ç½®
    auth_config = AuthApiKey(WEAVIATE_API_KEY)
    weaviate.connect.connection.has_grpc = False
    client = weaviate.Client(
        url=WEAVIATE_URL,
        auth_client_secret=auth_config,
        timeout_config=(5, 60),
        startup_period=None
    )
    client.batch.configure(batch_size=100, dynamic=True, timeout_retries=3)

    print("Weaviate ready:", client.is_ready())

    # âœ… å®šä¹‰ schema
    schema = {
        "class": "KnowledgeParagraph",
        "vectorizer": "none",
        "properties": [
            {"name": "part_id", "dataType": ["string"]},
            {"name": "knowledge_id", "dataType": ["string"]},
            {"name": "file_id", "dataType": ["string"]},
            {"name": "userid", "dataType": ["string"]},
            {"name": "username", "dataType": ["string"]},
            {"name": "digital_human_id", "dataType": ["string"]},
            {"name": "part_cntt", "dataType": ["text"]},
        ],
    }

    # âœ… åˆå§‹åŒ– schemaï¼ˆè‹¥å­˜åœ¨åˆ™å…ˆåˆ é™¤ï¼‰
    try:
        client.schema.delete_class("KnowledgeParagraph")
    except:
        pass
    client.schema.create_class(schema)
    print("Schema created âœ…")

    # âœ… å¯åŠ¨æ¶ˆè´¹
    consume_kafka_messages(client, KAFKA_TOPIC, KAFKA_BOOTSTRAP_SERVERS, KAFKA_GROUP_ID)
